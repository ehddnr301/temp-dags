import os
import subprocess
import logging
import gzip
import json
from datetime import datetime

import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd


def find_nulltype_paths(df: pd.DataFrame) -> list[str]:
    tbl = pa.Table.from_pandas(df, preserve_index=False)
    bad = []

    def walk(prefix: str, typ: pa.DataType):
        if pa.types.is_null(typ):                # list<null>, struct<â€¦nullâ€¦> ë“±
            bad.append(prefix.rstrip("."))
        elif pa.types.is_struct(typ):
            for field in typ:                    # pa.Field
                walk(f"{prefix}{field.name}.", field.type)
        elif pa.types.is_list(typ) or pa.types.is_large_list(typ):
            walk(prefix, typ.value_type)         # list ê°’ íƒ€ì… ê²€ì‚¬
        elif pa.types.is_map(typ):
            walk(prefix + "key.", typ.key_type)
            walk(prefix + "value.", typ.item_type)

    for field in tbl.schema:                     # â† Schema â†’ Field
        walk(f"{field.name}.", field.type)

    return sorted(set(bad))

NULL_PATHS = [
    "payload.forkee.language",
    "payload.forkee.mirror_url",
    "payload.forkee.topics",
    "payload.pages.summary",
    "payload.pull_request.active_lock_reason",
    "payload.pull_request.base.repo.mirror_url",
    "payload.pull_request.head.repo.mirror_url",
    "payload.pull_request.milestone.closed_at",
    "payload.pull_request.requested_teams.parent",
]

def strip_null_paths(event: dict) -> dict:
    """
    â‘  NULL_PATHS ì— ëª…ì‹œëœ í‚¤ ì‚­ì œ  
    â‘¡ dict/list ê°€ 'í…… ë¹ˆ' ìƒíƒœë¡œ ë‚¨ìœ¼ë©´ ìƒìœ„ì—ì„œë„ ì‚­ì œ
    """
    # 1) ì§€ì •ëœ ê²½ë¡œ í‚¤ ì œê±°
    for path in NULL_PATHS:
        cur = event
        keys = path.split(".")
        for k in keys[:-1]:
            cur = cur.get(k)
            if not isinstance(cur, dict):
                break
        else:
            cur.pop(keys[-1], None)

    # 2) ì¬ê·€ì ìœ¼ë¡œ ë¹ˆ ì»¨í…Œì´ë„ˆ(dict/list) ì œê±°
    def prune(obj):
        if isinstance(obj, dict):
            for k in list(obj.keys()):
                if prune(obj[k]):        # í•˜ìœ„ê°€ ëª¨ë‘ ë¹„ë©´ ì‚­ì œ
                    obj.pop(k, None)
            return len(obj) == 0
        if isinstance(obj, list):
            obj[:] = [v for v in obj if not prune(v)]
            return len(obj) == 0
        # ìŠ¤ì¹¼ë¼(null ì´ ì•„ë‹Œ ê°’) â†’ False
        return obj is None

    return event

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# MinIO í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
def get_minio_client():
    """MinIO í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    try:
        from minio import Minio
        from minio.error import S3Error

        print(os.getenv("AWS_ENDPOINT_URL", ""))
        print(os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"))
        print(os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"))
        print(os.getenv("MINIO_SECURE", "false").lower() == "true")
        
        return Minio(
            os.getenv("AWS_ENDPOINT_URL", "localhost:30090"),  # MinIO ì„œë²„ ì£¼ì†Œ
            access_key=os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),  # ì•¡ì„¸ìŠ¤ í‚¤
            secret_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),  # ì‹œí¬ë¦¿ í‚¤
            secure=os.getenv("MINIO_SECURE", "false").lower() == "true"  # HTTPS ì‚¬ìš© ì—¬ë¶€
        )
    except ImportError:
        logger.error("âŒ MinIO í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install minio'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        raise

def ensure_bucket_exists(bucket_name: str):
    """ë²„í‚·ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        from minio.error import S3Error
        client = get_minio_client()
        if not client.bucket_exists(bucket_name):
            client.make_bucket(bucket_name)
            logger.info(f"ğŸ“¦ ë²„í‚· ìƒì„± ì™„ë£Œ: {bucket_name}")
    except Exception as e:
        logger.error(f"âŒ ë²„í‚· ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def gharchive_url_for_hour(date_str: str, hour: int) -> str:
    """
    ì£¼ì–´ì§„ ë‚ ì§œ(date_str)ì™€ ì‹œê°„(hour)ì— í•´ë‹¹í•˜ëŠ” GH Archive URLì„ ë°˜í™˜.
    ex) 2025-07-12, 0ì‹œ -> "http://data.gharchive.org/2025-07-12-0.json.gz"
    """
    return f"http://data.gharchive.org/{date_str}-{hour}.json.gz"

def generate_urls_for_date(date_str: str) -> list[str]:
    """ë‚ ì§œ date_strì— ëŒ€í•œ 0ì‹œë¶€í„° 23ì‹œê¹Œì§€ 24ê°œì˜ URL ëª©ë¡ì„ ìƒì„±."""
    return [gharchive_url_for_hour(date_str, h) for h in range(24)]

def download_and_upload_to_minio(url: str, date: str, organization: str, hour: int):
    """wgetìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë°”ë¡œ MinIOì— ì—…ë¡œë“œ"""
    try:
        bucket_name = "gh-archive-raw"
        ensure_bucket_exists(bucket_name)
        
        # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
        temp_filename = f"/tmp/{date}-{hour}.json.gz"
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        
        # wgetìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        wget_cmd = [
            "wget", 
            "-O", temp_filename,
            "--timeout=30",
            "--tries=3",
            url
        ]
        
        logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
        result = subprocess.run(wget_cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"âŒ wget ì‹¤íŒ¨: {result.stderr}")
            return False
        
        # MinIOì— ì—…ë¡œë“œ
        client = get_minio_client()
        with open(temp_filename, 'rb') as file_data:
            client.put_object(
                bucket_name,
                object_name,
                file_data,
                length=os.path.getsize(temp_filename),
                content_type="application/gzip"
            )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_filename)
        
        logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {bucket_name}/{object_name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_filename):
            os.remove(temp_filename)
        return False

def download_all_hours(date: str, organization: str):
    """í•˜ë£¨ì¹˜ ëª¨ë“  ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  MinIOì— ì—…ë¡œë“œ"""
    urls = generate_urls_for_date(date)
    success_count = 0
    
    for hour, url in enumerate(urls):
        if download_and_upload_to_minio(url, date, organization, hour):
            success_count += 1
    
    logger.info(f"ğŸ“Š {date} - ì´ {len(urls)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ")
    return success_count

def process_and_save_to_delta(date: str, organization: str):
    """MinIOì—ì„œ gz íŒŒì¼ì„ ì½ì–´ì„œ Delta Lake í…Œì´ë¸”ë¡œ ì €ì¥ (ì‹œê°„ëŒ€ë³„ ê°œë³„ ì²˜ë¦¬)"""
    try:
        import pandas as pd
        from deltalake import DeltaTable, write_deltalake
    except ImportError:
        logger.error("âŒ pandas ë˜ëŠ” deltalakeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    bucket_name = "gh-archive-raw"
    delta_bucket_name = "gh-archive-delta"
    client = get_minio_client()
    
    # Delta Lake ë²„í‚· ìƒì„±
    ensure_bucket_exists(delta_bucket_name)
    
    # Delta Lake ê²½ë¡œ ì„¤ì • (MinIO S3 í˜¸í™˜)
    minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "localhost:30090")
    delta_path = f"s3://{delta_bucket_name}/{organization}/{date}"
    
    # S3 í˜¸í™˜ ì„¤ì •
    storage_options = {
        "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
        "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
        "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
        "AWS_REGION": "us-east-1",  # MinIOëŠ” ê¸°ë³¸ì ìœ¼ë¡œ us-east-1 ì‚¬ìš©
        "AWS_ALLOW_HTTP": "true",
        "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
    }
    
    total_rows = 0
    success_count = 0
    
    # 24ì‹œê°„ ë°ì´í„°ë¥¼ ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
    for hour in range(24):
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        temp_file = f"./tmp/{date}-{hour}.json.gz"
        
        try:
            # MinIOì—ì„œ gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            client.fget_object(bucket_name, object_name, temp_file)
            
            logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            
            # gz íŒŒì¼ ì••ì¶• í•´ì œ ë° JSON íŒŒì‹± (ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
            hour_data = []
            with gzip.open(temp_file, 'rt', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        if not data.get("org"):
                            continue
                        data = strip_null_paths(data)
                        hour_data.append(data)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_file)
            
            if hour_data:
                # í˜„ì¬ ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_hour = pd.DataFrame(hour_data)

                print(df_hour)
                print(df_hour.columns)
                print(df_hour.columns[df_hour.isna().any()])

                bad_fields = find_nulltype_paths(df_hour)
                print(bad_fields)
                
                # ì²« ë²ˆì§¸ ì‹œê°„ëŒ€ëŠ” ìƒˆë¡œ ìƒì„±, ì´í›„ëŠ” append ëª¨ë“œë¡œ ì¶”ê°€
                mode = "overwrite" if hour == 0 else "append"
                
                # Delta Lakeì— ì €ì¥
                write_deltalake(delta_path, df_hour, mode=mode, storage_options=storage_options)
                
                total_rows += len(df_hour)
                success_count += 1
                logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ - {len(df_hour)} í–‰ ì €ì¥ë¨")
            else:
                logger.warning(f"âš ï¸ {hour}ì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    if success_count == 0:
        logger.error("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logger.info(f"âœ… Delta Lake í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {delta_path} (ì´ {total_rows} í–‰, {success_count}ê°œ ì‹œê°„ëŒ€)")
    return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) != 4:
        print("ì‚¬ìš©ë²•: python gh_archive_daily_collector.py <YYYY-MM-DD> <organization> <type>")
        print("  type: 'download' (gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ) ë˜ëŠ” 'process' (ì••ì¶• í•´ì œ í›„ delta tableì— ì €ì¥)")
        sys.exit(1)
    
    date = sys.argv[1]
    organization = sys.argv[2]
    process_type = sys.argv[3]
    
    # íƒ€ì… ê²€ì¦
    if process_type not in ['download', 'process']:
        logger.error(f"âŒ ì˜ëª»ëœ íƒ€ì…: {process_type}. 'download' ë˜ëŠ” 'process'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    try:
        datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {date}")
        sys.exit(1)
    
    logger.info(f"ğŸš€ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì¡°ì§: {organization}, íƒ€ì…: {process_type}")
    
    if process_type == 'download':
        success_count = download_all_hours(date, organization)
        logger.info(f"âœ… {date} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - {success_count}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
    elif process_type == 'process':
        logger.info(f"ğŸ”„ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì••ì¶• í•´ì œ ë° delta table ì €ì¥")
        success = process_and_save_to_delta(date, organization)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)

if __name__ == "__main__":
    main()
