import os
import pandas as pd
import requests
import json
import gzip
import io
from datetime import datetime, timedelta
from deltalake import write_deltalake
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# MinIO í™˜ê²½ ë³€ìˆ˜
os.environ["AWS_ACCESS_KEY_ID"] = "minioadmin"
os.environ["AWS_SECRET_ACCESS_KEY"] = "minioadmin"
os.environ["AWS_ENDPOINT_URL"] = "http://minio:9000"
os.environ["AWS_ALLOW_HTTP"] = "true"
os.environ["AWS_CONDITIONAL_PUT"] = "etag"

def gharchive_url_for_hour(date_str: str, hour: int) -> str:
    """
    ì£¼ì–´ì§„ ë‚ ì§œ(date_str)ì™€ ì‹œê°„(hour)ì— í•´ë‹¹í•˜ëŠ” GH Archive URLì„ ë°˜í™˜.
    ex) 2025-07-12, 0ì‹œ -> "http://data.gharchive.org/2025-07-12-0.json.gz"
    """
    return f"http://data.gharchive.org/{date_str}-{hour}.json.gz"

def generate_urls_for_date(date_str: str) -> list[str]:
    """ë‚ ì§œ date_strì— ëŒ€í•œ 0ì‹œë¶€í„° 23ì‹œê¹Œì§€ 24ê°œì˜ URL ëª©ë¡ì„ ìƒì„±."""
    return [gharchive_url_for_hour(date_str, h) for h in range(24)]

def fetch_organization_data(date: str, organization: str) -> pd.DataFrame:
    """íŠ¹ì • ë‚ ì§œì™€ organizationì˜ GH Archive ë°ì´í„° ìˆ˜ì§‘"""
    all_events = []
    
    for url in generate_urls_for_date(date):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # gzip ì••ì¶• í•´ì œ
            with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:
                for line in f:
                    try:
                        event = json.loads(line.decode('utf-8'))
                        # eventê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                        if not isinstance(event, dict):
                            continue
                        # organization ê´€ë ¨ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
                        if event.get("org", {}).get("login") == organization:
                            all_events.append(event)
                    except json.JSONDecodeError:
                        continue
            
            time.sleep(0.1)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
            
        except requests.RequestException as e:
            logger.error(f"âŒ {url} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    if not all_events:
        return pd.DataFrame()
    
    df = pd.DataFrame(all_events)
    df['date'] = date
    df['created_at'] = pd.to_datetime(df['created_at'])
    
    logger.info(f"ğŸ“Š {date} - {len(df)}ê°œ ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
    return df

def write_to_delta(df: pd.DataFrame, date: str, organization: str):
    """ë¸íƒ€ í…Œì´ë¸”ì— ë°ì´í„° ì“°ê¸°"""
    if df.empty:
        logger.warning(f"âš ï¸ {date} - ë¹ˆ ë°ì´í„°")
        return
    
    try:
        delta_path = f"s3://gh-archive/gh-archive-{organization.lower()}"
        partition_path = f"{delta_path}/date={date}"
        
        write_deltalake(partition_path, df, mode="overwrite", partition_by=["date"])
        logger.info(f"ğŸ’¾ {date} ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ {date} ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) != 3:
        print("ì‚¬ìš©ë²•: python gh_archive_daily_collector.py <YYYY-MM-DD> <organization>")
        print("ì˜ˆì‹œ: python gh_archive_daily_collector.py 2024-01-15 apache")
        sys.exit(1)
    
    date = sys.argv[1]
    organization = sys.argv[2]
    
    # ë‚ ì§œ í˜•ì‹ ê²€ì¦
    try:
        datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {date}. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    logger.info(f"ğŸš€ {date} {organization} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    # ë°ì´í„° ìˆ˜ì§‘
    df = fetch_organization_data(date, organization)
    
    # ë¸íƒ€ í…Œì´ë¸”ì— ì €ì¥
    write_to_delta(df, date, organization)
    
    logger.info(f"âœ… {date} {organization} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")

if __name__ == "__main__":
    main()
