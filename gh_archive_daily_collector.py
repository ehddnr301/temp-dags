import os
import subprocess
import logging
import gzip
import json
from datetime import datetime
import ast
import re
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.parquet as pq
import pyarrow.fs
from deltalake import DeltaTable, write_deltalake

import pandas as pd

def _convert_all_columns_to_string(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì˜ ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    for col in df.columns:
        df[col] = df[col].astype(str)
    return df

def _to_snake_case(name: str) -> str:
    """CamelCaseë¥¼ snake_caseë¡œ ë³€í™˜"""
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    s2 = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    s3 = s2.replace('-', '_')
    return s3

def _safe_json_parse(json_str):
    """JSON ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±"""
    if pd.isna(json_str) or json_str is None or json_str == "" or json_str == "None":
        return None
    if isinstance(json_str, str):
        try:
            if json_str.strip().startswith('{'):
                return ast.literal_eval(json_str)
            return json.loads(json_str)
        except:
            return None
    return json_str

def _create_struct_from_dict_list(dict_list, struct_name):
    """ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ Arrow struct íƒ€ì…ì„ ìƒì„±"""
    if not dict_list or all(d is None for d in dict_list):
        return pa.array([None] * len(dict_list))
    
    sample_dict = next((d for d in dict_list if d is not None), None)
    if sample_dict is None:
        return pa.array([None] * len(dict_list))
    
    # GitHub Archive í‘œì¤€ ìŠ¤í‚¤ë§ˆ
    if struct_name == 'actor':
        schema = pa.struct([
            ('id', pa.int64()),
            ('login', pa.string()),
            ('display_login', pa.string()),
            ('gravatar_id', pa.string()),
            ('url', pa.string()),
            ('avatar_url', pa.string())
        ])
    elif struct_name == 'repo':
        schema = pa.struct([
            ('id', pa.int64()),
            ('name', pa.string()),
            ('url', pa.string())
        ])
    elif struct_name == 'org':
        schema = pa.struct([
            ('id', pa.int64()),
            ('login', pa.string()),
            ('gravatar_id', pa.string()),
            ('url', pa.string()),
            ('avatar_url', pa.string())
        ])
    else:  # payloadëŠ” ë™ì ìœ¼ë¡œ ì²˜ë¦¬
        fields = []
        for key, value in sample_dict.items():
            if isinstance(value, int):
                fields.append((key, pa.int64()))
            elif isinstance(value, bool):
                fields.append((key, pa.bool_()))
            elif isinstance(value, list):
                fields.append((key, pa.string()))
            else:
                fields.append((key, pa.string()))
        schema = pa.struct(fields)
    
    # ë”•ì…”ë„ˆë¦¬ë¥¼ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
    struct_data = []
    for d in dict_list:
        if d is None:
            struct_data.append(None)
        else:
            converted = {}
            for field in schema:
                field_name = field.name
                field_type = field.type
                value = d.get(field_name)
                
                if value is None:
                    converted[field_name] = None
                elif pa.types.is_integer(field_type):
                    try:
                        converted[field_name] = int(value) if value != "" else None
                    except:
                        converted[field_name] = None
                elif pa.types.is_boolean(field_type):
                    converted[field_name] = bool(value) if value != "" else None
                else:
                    converted[field_name] = str(value) if value != "" else None
            
            struct_data.append(converted)
    
    return pa.array(struct_data, type=schema)

def _convert_dtypes_arrow(arrow_table):
    """Arrow í…Œì´ë¸”ì˜ íƒ€ì… ë³€í™˜"""
    columns = {}
    
    for field in arrow_table.schema:
        col_name = field.name
        col_data = arrow_table.column(col_name)
        
        if col_name.startswith("__index_level"):
            continue
            
        # boolean íƒ€ì… ë³€í™˜
        if col_name == 'public':
            try:
                if pa.types.is_string(field.type):
                    bool_col = pc.equal(col_data, pa.scalar("True"))
                    columns[col_name] = bool_col
                else:
                    columns[col_name] = col_data
            except Exception:
                columns[col_name] = col_data
        
        # datetime íƒ€ì… ë³€í™˜
        elif col_name == 'created_at':
            try:
                if pa.types.is_string(field.type):
                    timestamp_col = pc.strptime(col_data, format='%Y-%m-%dT%H:%M:%SZ', unit='s')
                    columns[col_name] = timestamp_col
                else:
                    columns[col_name] = col_data
            except Exception:
                columns[col_name] = col_data
        
        # payloadì™€ orgëŠ” JSON ë¬¸ìì—´ë¡œ ìœ ì§€
        elif col_name in ['payload', 'org']:
            columns[col_name] = col_data
        
        # actor, repoëŠ” structë¡œ ë³€í™˜
        elif col_name in ['actor', 'repo']:
            try:
                pandas_col = col_data.to_pandas()
                dict_list = [_safe_json_parse(x) for x in pandas_col]
                struct_col = _create_struct_from_dict_list(dict_list, col_name)
                columns[col_name] = struct_col
            except Exception as e:
                logger.warning(f"âš ï¸ {col_name} struct ë³€í™˜ ì‹¤íŒ¨: {e}")
                columns[col_name] = col_data
        
        else:
            if pa.types.is_null(field.type):
                null_col = pa.array([None] * len(col_data), type=pa.string())
                columns[col_name] = null_col
            else:
                columns[col_name] = col_data
    
    return pa.table(columns)

def _filter_by_organization(df: pd.DataFrame, target_logins: list) -> pd.DataFrame:
    """ì¡°ì§ í•„í„°ë§ í•¨ìˆ˜"""
    if 'org' not in df.columns or len(df) == 0:
        return df
    
    def safe_dict_parse(x):
        if pd.isna(x) or x is None:
            return None
        if isinstance(x, str):
            try:
                return ast.literal_eval(x)
            except (ValueError, SyntaxError):
                return None
        return x
    
    df['org_parsed'] = df['org'].apply(safe_dict_parse)
    filtered_df = df[df['org_parsed'].apply(lambda x: x and isinstance(x, dict) and x.get('login') in target_logins)]
    
    if len(filtered_df) > 0:
        filtered_df = filtered_df.drop('org_parsed', axis=1)
    
    return filtered_df


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# MinIO í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
def _get_minio_client():
    """MinIO í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    try:
        from minio import Minio
        from minio.error import S3Error  # noqa: F401

        return Minio(
            os.getenv("AWS_ENDPOINT_URL", "minio:9000"),  # MinIO ì„œë²„ ì£¼ì†Œ
            access_key=os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),  # ì•¡ì„¸ìŠ¤ í‚¤
            secret_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),  # ì‹œí¬ë¦¿ í‚¤
            secure=os.getenv("MINIO_SECURE", "false").lower() == "true"  # HTTPS ì‚¬ìš© ì—¬ë¶€
        )
    except ImportError:
        logger.error("âŒ MinIO í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install minio'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        raise

def _ensure_bucket_exists(bucket_name: str):
    """ë²„í‚·ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        from minio.error import S3Error  # noqa: F401
        client = _get_minio_client()
        if not client.bucket_exists(bucket_name):
            client.make_bucket(bucket_name)
            logger.info(f"ğŸ“¦ ë²„í‚· ìƒì„± ì™„ë£Œ: {bucket_name}")
    except Exception as e:
        logger.error(f"âŒ ë²„í‚· ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def _gharchive_url_for_hour(date_str: str, hour: int) -> str:
    """
    ì£¼ì–´ì§„ ë‚ ì§œ(date_str)ì™€ ì‹œê°„(hour)ì— í•´ë‹¹í•˜ëŠ” GH Archive URLì„ ë°˜í™˜.
    ex) 2025-07-12, 0ì‹œ -> "http://data.gharchive.org/2025-07-12-0.json.gz"
    """
    return f"http://data.gharchive.org/{date_str}-{hour}.json.gz"

def _generate_urls_for_date(date_str: str) -> list[str]:
    """ë‚ ì§œ date_strì— ëŒ€í•œ 0ì‹œë¶€í„° 23ì‹œê¹Œì§€ 24ê°œì˜ URL ëª©ë¡ì„ ìƒì„±."""
    return [_gharchive_url_for_hour(date_str, h) for h in range(24)]

def _download_and_upload_to_minio(url: str, date: str, organization: str, hour: int):
    """wgetìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë°”ë¡œ MinIOì— ì—…ë¡œë“œ"""
    try:
        bucket_name = "gh-archive-raw"
        _ensure_bucket_exists(bucket_name)
        
        # tmp ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("./tmp", exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
        temp_filename = f"./tmp/{date}-{hour}.json.gz"
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        
        # wgetìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        wget_cmd = [
            "wget", 
            "-O", temp_filename,
            "--timeout=30",
            "--tries=3",
            url
        ]
        
        logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
        result = subprocess.run(wget_cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"âŒ wget ì‹¤íŒ¨: {result.stderr}")
            return False
        
        # MinIOì— ì—…ë¡œë“œ
        client = _get_minio_client()
        with open(temp_filename, 'rb') as file_data:
            client.put_object(
                bucket_name,
                object_name,
                file_data,
                length=os.path.getsize(temp_filename),
                content_type="application/gzip"
            )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_filename)
        
        logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {bucket_name}/{object_name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_filename):
            os.remove(temp_filename)
        return False

def download_all_hours(date: str, organization: str):
    """í•˜ë£¨ì¹˜ ëª¨ë“  ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  MinIOì— ì—…ë¡œë“œ"""
    urls = _generate_urls_for_date(date)
    success_count = 0
    
    for hour, url in enumerate(urls):
        if _download_and_upload_to_minio(url, date, organization, hour):
            success_count += 1
    
    logger.info(f"ğŸ“Š {date} - ì´ {len(urls)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ")
    return success_count

def process_and_save_to_delta(date: str, organization: str):
    """MinIOì—ì„œ gz íŒŒì¼ì„ ì½ì–´ì„œ Delta Lake í…Œì´ë¸”ë¡œ ì €ì¥ (ì‹œê°„ëŒ€ë³„ ê°œë³„ ì²˜ë¦¬)"""
    try:
        import pandas as pd
        from deltalake import write_deltalake
    except ImportError:
        logger.error("âŒ pandas ë˜ëŠ” deltalakeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    bucket_name = "gh-archive-raw"
    delta_bucket_name = "gh-archive-delta"
    client = _get_minio_client()
    
    # Delta Lake ë²„í‚· ìƒì„±
    _ensure_bucket_exists(delta_bucket_name)
    
    # Delta Lake ê²½ë¡œ ì„¤ì • (MinIO S3 í˜¸í™˜)
    minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
    delta_path = f"s3://{delta_bucket_name}/{organization}/{date}"
    
    # S3 í˜¸í™˜ ì„¤ì •
    storage_options = {
        "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
        "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
        "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
        "AWS_REGION": "us-east-1",  # MinIOëŠ” ê¸°ë³¸ì ìœ¼ë¡œ us-east-1 ì‚¬ìš©
        "AWS_ALLOW_HTTP": "true",
        "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
    }
    
    total_rows = 0
    success_count = 0
    
    # tmp ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./tmp", exist_ok=True)
    
    # 24ì‹œê°„ ë°ì´í„°ë¥¼ ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
    for hour in range(24):
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        temp_file = f"./tmp/{date}-{hour}.json.gz"
        
        try:
            # MinIOì—ì„œ gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            client.fget_object(bucket_name, object_name, temp_file)
            
            logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            
            # gz íŒŒì¼ ì••ì¶• í•´ì œ ë° JSON íŒŒì‹± (ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
            hour_data = []
            with gzip.open(temp_file, 'rt', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        # ê¸°ë³¸ í•„í„°ë§: orgê°€ ìˆëŠ” ë°ì´í„°ë§Œ ì²˜ë¦¬
                        if not data.get("org"):
                            continue
                        hour_data.append(data)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_file)
            
            if hour_data:
                # í˜„ì¬ ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_hour = pd.DataFrame(hour_data)
                
                # ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                df_hour = _convert_all_columns_to_string(df_hour)
                
                # ì²« ë²ˆì§¸ ì‹œê°„ëŒ€ëŠ” ìƒˆë¡œ ìƒì„±, ì´í›„ëŠ” append ëª¨ë“œë¡œ ì¶”ê°€
                mode = "overwrite" if hour == 0 else "append"
                print(df_hour.columns)
                # Delta Lakeì— ì €ì¥
                write_deltalake(delta_path, df_hour, mode=mode, storage_options=storage_options)
                
                # Delta Lake ì €ì¥ ì„±ê³µ í›„ MinIO raw ë²„í‚·ì—ì„œ ì›ë³¸ íŒŒì¼ ì‚­ì œ
                try:
                    client.remove_object(bucket_name, object_name)
                    logger.info(f"ğŸ—‘ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {bucket_name}/{object_name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                
                total_rows += len(df_hour)
                success_count += 1
                logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ - {len(df_hour)} í–‰ ì €ì¥ë¨")
            else:
                logger.warning(f"âš ï¸ {hour}ì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    if success_count == 0:
        logger.error("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logger.info(f"âœ… Delta Lake í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {delta_path} (ì´ {total_rows} í–‰, {success_count}ê°œ ì‹œê°„ëŒ€)")
    return True

def split_filtered_data_by_organization(date: str, organization: str, target_logins: list):
    """í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì¡°ì§ë³„ë¡œ ë¶„ë¦¬í•´ì„œ ì €ì¥"""
    try:
        # MinIO ì„¤ì •
        minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
        storage_options = {
            "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
            "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
            "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
            "AWS_REGION": "us-east-1",
            "AWS_ALLOW_HTTP": "true",
            "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
        }
        
        # í•„í„°ë§ëœ ë°ì´í„° ê²½ë¡œ
        source_path = f"s3://gh-archive-delta/filtered_{organization}/{date}"
        
        logger.info(f"ğŸ”„ ì¡°ì§ë³„ ë¶„ë¦¬ ì‹œì‘: {source_path}")
        
        # í•„í„°ë§ëœ ë°ì´í„° ì½ê¸°
        dt = DeltaTable(source_path, storage_options=storage_options)
        df = dt.to_pandas()
        
        logger.info(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)} í–‰")
        
        # org ì»¬ëŸ¼ íŒŒì‹±
        def safe_dict_parse(x):
            if pd.isna(x) or x is None:
                return None
            if isinstance(x, str):
                try:
                    return ast.literal_eval(x)
                except (ValueError, SyntaxError):
                    return None
            return x
        
        df['org_parsed'] = df['org'].apply(safe_dict_parse)
        
        # ê° target_loginë³„ë¡œ ë¶„ë¦¬ ì €ì¥
        success_count = 0
        for login in target_logins:
            org_df = df[df['org_parsed'].apply(lambda x: x and isinstance(x, dict) and x.get('login') == login)]
            
            if len(org_df) > 0:
                # org_parsed ì»¬ëŸ¼ ì œê±° ë° string ë³€í™˜
                org_df_clean = org_df.drop('org_parsed', axis=1)
                org_df_clean = _convert_all_columns_to_string(org_df_clean)
                
                # ì¡°ì§ë³„ ì €ì¥ ê²½ë¡œ
                output_path = f"s3://gh-archive-delta/org_{login}/{date}"
                
                # Delta Lakeì— ì €ì¥
                write_deltalake(output_path, org_df_clean, mode="overwrite", storage_options=storage_options)
                logger.info(f"âœ… {login}: {len(org_df_clean)}í–‰ ì €ì¥ ì™„ë£Œ")
                success_count += 1
            else:
                logger.warning(f"âš ï¸ {login}: ë°ì´í„° ì—†ìŒ")
        
        logger.info(f"âœ… ì¡°ì§ë³„ ë¶„ë¦¬ ì™„ë£Œ: {success_count}/{len(target_logins)}ê°œ ì¡°ì§")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ ì¡°ì§ë³„ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def optimize_schema_for_organizations(date: str, organization: str, target_logins: list):
    """ì¡°ì§ë³„ ë°ì´í„°ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ìµœì í™”í•˜ì—¬ ì €ì¥"""
    try:
        # MinIO ì„¤ì •
        minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
        storage_options = {
            "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
            "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
            "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
            "AWS_REGION": "us-east-1",
            "AWS_ALLOW_HTTP": "true",
            "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
        }
        
        logger.info(f"âš¡ {date} ìŠ¤í‚¤ë§ˆ ìµœì í™” ì‹œì‘ - ëŒ€ìƒ ì¡°ì§: {target_logins}")
        
        success_count = 0
        for login in target_logins:
            try:
                # ì†ŒìŠ¤ ê²½ë¡œ (ë¶„ë¦¬ëœ ì¡°ì§ ë°ì´í„°)
                source_path = f"s3://gh-archive-delta/org_{login}/{date}"
                logger.info(f"ğŸ“‚ ì½ëŠ” ì¤‘: {source_path}")
                
                # Arrow í…Œì´ë¸”ë¡œ ì§ì ‘ ì½ê¸°
                dt = DeltaTable(source_path, storage_options=storage_options)
                arrow_table = dt.to_pyarrow_table()
                
                logger.info(f"ğŸ“Š {login} ì›ë³¸ ë°ì´í„°: {len(arrow_table)} í–‰")
                logger.info(f"ğŸ“‹ ì›ë³¸ ìŠ¤í‚¤ë§ˆ: {len(arrow_table.schema)} ì»¬ëŸ¼")
                
                # íƒ€ì… ë³€í™˜
                arrow_table_converted = _convert_dtypes_arrow(arrow_table)
                logger.info(f"ğŸ“‹ ë³€í™˜ëœ ìŠ¤í‚¤ë§ˆ: {len(arrow_table_converted.schema)} ì»¬ëŸ¼")
                
                # base_date ì»¬ëŸ¼ ì¶”ê°€ (íŒŒí‹°ì…˜ìš©)
                base_date_col = pa.array([date] * len(arrow_table_converted), type=pa.string())
                arrow_table_with_date = arrow_table_converted.append_column('base_date', base_date_col)
                
                # ìµœì í™”ëœ ì €ì¥ ê²½ë¡œ (snake_case ì²˜ë¦¬)
                login_snake = _to_snake_case(login)
                output_path = f"s3://gh-archive-delta/optimized_org_{login_snake}/"
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                try:
                    existing_table = DeltaTable(output_path, storage_options=storage_options)
                    table_exists = True
                    logger.info(f"ğŸ“‹ ê¸°ì¡´ í…Œì´ë¸” ë°œê²¬: {output_path}")
                except Exception:
                    table_exists = False
                    logger.info(f"ğŸ“‹ ìƒˆ í…Œì´ë¸” ìƒì„±: {output_path}")
                
                if table_exists:
                    # í•´ë‹¹ ë‚ ì§œ íŒŒí‹°ì…˜ ì‚­ì œ í›„ ì¶”ê°€ (ë©±ë“±ì„± ë³´ì¥)
                    logger.info(f"ğŸ”„ {date} íŒŒí‹°ì…˜ ë°ì´í„° êµì²´ ì¤‘...")
                    existing_table.delete(f"base_date = '{date}'")
                    
                    write_deltalake(
                        output_path,
                        arrow_table_with_date,
                        mode="append",
                        storage_options=storage_options,
                        partition_by=["base_date"],
                        schema_mode="merge"  # ìŠ¤í‚¤ë§ˆ ì§„í™” í—ˆìš©
                    )
                else:
                    # ì²« ë²ˆì§¸ ì‹¤í–‰ - í…Œì´ë¸” ìƒì„±
                    write_deltalake(
                        output_path,
                        arrow_table_with_date,
                        mode="overwrite",
                        storage_options=storage_options,
                        partition_by=["base_date"]
                    )
                
                logger.info(f"âœ… {login}: ìŠ¤í‚¤ë§ˆ ìµœì í™” ì™„ë£Œ â†’ {output_path}")
                success_count += 1
                
            except Exception as e:
                logger.error(f"âŒ {login} ìŠ¤í‚¤ë§ˆ ìµœì í™” ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… ìŠ¤í‚¤ë§ˆ ìµœì í™” ì™„ë£Œ: {success_count}/{len(target_logins)}ê°œ ì¡°ì§")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í‚¤ë§ˆ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

def filter_delta_table_by_organization(date: str, organization: str, target_logins: list):
    """Delta Lake í…Œì´ë¸”ì—ì„œ ì¡°ì§ë³„ë¡œ ë°ì´í„° í•„í„°ë§"""
    try:
        # MinIO ì„¤ì •
        minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
        storage_options = {
            "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
            "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
            "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
            "AWS_REGION": "us-east-1",
            "AWS_ALLOW_HTTP": "true",
            "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
        }
        
        # ì›ë³¸ ë° í•„í„°ë§ëœ í…Œì´ë¸” ê²½ë¡œ
        source_path = f"s3://gh-archive-delta/{organization}/{date}"
        filtered_path = f"s3://gh-archive-delta/filtered_{organization}/{date}"
        
        logger.info(f"ğŸ” í•„í„°ë§ ì‹œì‘: {source_path} -> {filtered_path}")
        
        # Delta Lake í…Œì´ë¸” ì½ê¸°
        dt = DeltaTable(source_path, storage_options=storage_options)
        df = dt.to_pandas()
        
        logger.info(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)} í–‰")
        
        # ì¡°ì§ë³„ í•„í„°ë§
        filtered_df = _filter_by_organization(df, target_logins)
        
        if len(filtered_df) > 0:
            # ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            filtered_df = _convert_all_columns_to_string(filtered_df)
            
            # í•„í„°ë§ëœ ë°ì´í„° ì €ì¥
            write_deltalake(filtered_path, filtered_df, mode="overwrite", storage_options=storage_options)
            logger.info(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_df)} í–‰ ì €ì¥ë¨")
            return True
        else:
            logger.warning("âš ï¸ í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        logger.error(f"âŒ í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) < 4:
        print("ì‚¬ìš©ë²•: python gh_archive_daily_collector.py <YYYY-MM-DD> <organization> <type> [target_logins]")
        print("  type: 'download', 'process', 'filter', 'split'")
        print("    - download: gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
        print("    - process: ì••ì¶• í•´ì œ í›„ delta tableì— ì €ì¥")
        print("    - filter: ì¡°ì§ë³„ í•„í„°ë§")
        print("    - split: í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì¡°ì§ë³„ë¡œ ë¶„ë¦¬")
        print("    - optimize: ì¡°ì§ë³„ ë°ì´í„°ì˜ ìŠ¤í‚¤ë§ˆ ìµœì í™”")
        print("  target_logins (filter/split/optimizeìš©): ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì¡°ì§ëª… (ì˜ˆ: CausalInferenceLab,Pseudo-Lab,apache)")
        sys.exit(1)
    
    date = sys.argv[1]
    organization = sys.argv[2]
    process_type = sys.argv[3]
    
    # íƒ€ì… ê²€ì¦
    if process_type not in ['download', 'process', 'filter', 'split', 'optimize']:
        logger.error(f"âŒ ì˜ëª»ëœ íƒ€ì…: {process_type}. 'download', 'process', 'filter', 'split' ë˜ëŠ” 'optimize'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    try:
        datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {date}")
        sys.exit(1)
    
    logger.info(f"ğŸš€ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì¡°ì§: {organization}, íƒ€ì…: {process_type}")
    
    if process_type == 'download':
        success_count = download_all_hours(date, organization)
        logger.info(f"âœ… {date} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - {success_count}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
    elif process_type == 'process':
        logger.info(f"ğŸ”„ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì••ì¶• í•´ì œ ë° delta table ì €ì¥")
        success = process_and_save_to_delta(date, organization)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)
    elif process_type == 'filter':
        if len(sys.argv) < 5:
            logger.error("âŒ filter íƒ€ì…ì—ëŠ” target_logins íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        target_logins = sys.argv[4].split(',')
        logger.info(f"ğŸ” {date} ë°ì´í„° í•„í„°ë§ ì‹œì‘ - ëŒ€ìƒ ì¡°ì§: {target_logins}")
        success = filter_delta_table_by_organization(date, organization, target_logins)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° í•„í„°ë§ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° í•„í„°ë§ ì‹¤íŒ¨")
            sys.exit(1)
    elif process_type == 'split':
        if len(sys.argv) < 5:
            logger.error("âŒ split íƒ€ì…ì—ëŠ” target_logins íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        target_logins = sys.argv[4].split(',')
        logger.info(f"ğŸ”„ {date} ë°ì´í„° ì¡°ì§ë³„ ë¶„ë¦¬ ì‹œì‘ - ëŒ€ìƒ ì¡°ì§: {target_logins}")
        success = split_filtered_data_by_organization(date, organization, target_logins)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ì¡°ì§ë³„ ë¶„ë¦¬ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ì¡°ì§ë³„ ë¶„ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)
    elif process_type == 'optimize':
        if len(sys.argv) < 5:
            logger.error("âŒ optimize íƒ€ì…ì—ëŠ” target_logins íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        target_logins = sys.argv[4].split(',')
        logger.info(f"âš¡ {date} ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìµœì í™” ì‹œì‘ - ëŒ€ìƒ ì¡°ì§: {target_logins}")
        success = optimize_schema_for_organizations(date, organization, target_logins)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìµœì í™” ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìµœì í™” ì‹¤íŒ¨")
            sys.exit(1)

if __name__ == "__main__":
    main()
