import os
import subprocess
import logging
import gzip
import json
from datetime import datetime

import pandas as pd

def _convert_all_columns_to_string(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì˜ ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    for col in df.columns:
        df[col] = df[col].astype(str)
    return df


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# MinIO í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
def _get_minio_client():
    """MinIO í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    try:
        from minio import Minio
        from minio.error import S3Error

        return Minio(
            os.getenv("AWS_ENDPOINT_URL", "minio:9000"),  # MinIO ì„œë²„ ì£¼ì†Œ
            access_key=os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),  # ì•¡ì„¸ìŠ¤ í‚¤
            secret_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),  # ì‹œí¬ë¦¿ í‚¤
            secure=os.getenv("MINIO_SECURE", "false").lower() == "true"  # HTTPS ì‚¬ìš© ì—¬ë¶€
        )
    except ImportError:
        logger.error("âŒ MinIO í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install minio'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        raise

def _ensure_bucket_exists(bucket_name: str):
    """ë²„í‚·ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        from minio.error import S3Error
        client = _get_minio_client()
        if not client.bucket_exists(bucket_name):
            client.make_bucket(bucket_name)
            logger.info(f"ğŸ“¦ ë²„í‚· ìƒì„± ì™„ë£Œ: {bucket_name}")
    except Exception as e:
        logger.error(f"âŒ ë²„í‚· ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def _gharchive_url_for_hour(date_str: str, hour: int) -> str:
    """
    ì£¼ì–´ì§„ ë‚ ì§œ(date_str)ì™€ ì‹œê°„(hour)ì— í•´ë‹¹í•˜ëŠ” GH Archive URLì„ ë°˜í™˜.
    ex) 2025-07-12, 0ì‹œ -> "http://data.gharchive.org/2025-07-12-0.json.gz"
    """
    return f"http://data.gharchive.org/{date_str}-{hour}.json.gz"

def _generate_urls_for_date(date_str: str) -> list[str]:
    """ë‚ ì§œ date_strì— ëŒ€í•œ 0ì‹œë¶€í„° 23ì‹œê¹Œì§€ 24ê°œì˜ URL ëª©ë¡ì„ ìƒì„±."""
    return [_gharchive_url_for_hour(date_str, h) for h in range(24)]

def _download_and_upload_to_minio(url: str, date: str, organization: str, hour: int):
    """wgetìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë°”ë¡œ MinIOì— ì—…ë¡œë“œ"""
    try:
        bucket_name = "gh-archive-raw"
        _ensure_bucket_exists(bucket_name)
        
        # tmp ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("./tmp", exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
        temp_filename = f"./tmp/{date}-{hour}.json.gz"
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        
        # wgetìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        wget_cmd = [
            "wget", 
            "-O", temp_filename,
            "--timeout=30",
            "--tries=3",
            url
        ]
        
        logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
        result = subprocess.run(wget_cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"âŒ wget ì‹¤íŒ¨: {result.stderr}")
            return False
        
        # MinIOì— ì—…ë¡œë“œ
        client = _get_minio_client()
        with open(temp_filename, 'rb') as file_data:
            client.put_object(
                bucket_name,
                object_name,
                file_data,
                length=os.path.getsize(temp_filename),
                content_type="application/gzip"
            )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_filename)
        
        logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {bucket_name}/{object_name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_filename):
            os.remove(temp_filename)
        return False

def download_all_hours(date: str, organization: str):
    """í•˜ë£¨ì¹˜ ëª¨ë“  ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  MinIOì— ì—…ë¡œë“œ"""
    urls = _generate_urls_for_date(date)
    success_count = 0
    
    for hour, url in enumerate(urls):
        if _download_and_upload_to_minio(url, date, organization, hour):
            success_count += 1
    
    logger.info(f"ğŸ“Š {date} - ì´ {len(urls)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ")
    return success_count

def process_and_save_to_delta(date: str, organization: str):
    """MinIOì—ì„œ gz íŒŒì¼ì„ ì½ì–´ì„œ Delta Lake í…Œì´ë¸”ë¡œ ì €ì¥ (ì‹œê°„ëŒ€ë³„ ê°œë³„ ì²˜ë¦¬)"""
    try:
        import pandas as pd
        from deltalake import DeltaTable, write_deltalake
    except ImportError:
        logger.error("âŒ pandas ë˜ëŠ” deltalakeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    bucket_name = "gh-archive-raw"
    delta_bucket_name = "gh-archive-delta"
    client = _get_minio_client()
    
    # Delta Lake ë²„í‚· ìƒì„±
    _ensure_bucket_exists(delta_bucket_name)
    
    # Delta Lake ê²½ë¡œ ì„¤ì • (MinIO S3 í˜¸í™˜)
    minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
    delta_path = f"s3://{delta_bucket_name}/{organization}/{date}"
    
    # S3 í˜¸í™˜ ì„¤ì •
    storage_options = {
        "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
        "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
        "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
        "AWS_REGION": "us-east-1",  # MinIOëŠ” ê¸°ë³¸ì ìœ¼ë¡œ us-east-1 ì‚¬ìš©
        "AWS_ALLOW_HTTP": "true",
        "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
    }
    
    total_rows = 0
    success_count = 0
    
    # tmp ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./tmp", exist_ok=True)
    
    # 24ì‹œê°„ ë°ì´í„°ë¥¼ ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
    for hour in range(24):
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        temp_file = f"./tmp/{date}-{hour}.json.gz"
        
        try:
            # MinIOì—ì„œ gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            client.fget_object(bucket_name, object_name, temp_file)
            
            logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            
            # gz íŒŒì¼ ì••ì¶• í•´ì œ ë° JSON íŒŒì‹± (ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
            hour_data = []
            with gzip.open(temp_file, 'rt', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        if not data.get("org"):
                            continue
                        hour_data.append(data)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_file)
            
            if hour_data:
                # í˜„ì¬ ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_hour = pd.DataFrame(hour_data)
                
                # ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                df_hour = _convert_all_columns_to_string(df_hour)
                
                # ì²« ë²ˆì§¸ ì‹œê°„ëŒ€ëŠ” ìƒˆë¡œ ìƒì„±, ì´í›„ëŠ” append ëª¨ë“œë¡œ ì¶”ê°€
                mode = "overwrite" if hour == 0 else "append"
                print(df_hour.columns)
                # Delta Lakeì— ì €ì¥
                write_deltalake(delta_path, df_hour, mode=mode, storage_options=storage_options)
                
                # Delta Lake ì €ì¥ ì„±ê³µ í›„ MinIO raw ë²„í‚·ì—ì„œ ì›ë³¸ íŒŒì¼ ì‚­ì œ
                try:
                    client.remove_object(bucket_name, object_name)
                    logger.info(f"ğŸ—‘ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {bucket_name}/{object_name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                
                total_rows += len(df_hour)
                success_count += 1
                logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ - {len(df_hour)} í–‰ ì €ì¥ë¨")
            else:
                logger.warning(f"âš ï¸ {hour}ì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    if success_count == 0:
        logger.error("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logger.info(f"âœ… Delta Lake í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {delta_path} (ì´ {total_rows} í–‰, {success_count}ê°œ ì‹œê°„ëŒ€)")
    return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) != 4:
        print("ì‚¬ìš©ë²•: python gh_archive_daily_collector.py <YYYY-MM-DD> <organization> <type>")
        print("  type: 'download' (gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ) ë˜ëŠ” 'process' (ì••ì¶• í•´ì œ í›„ delta tableì— ì €ì¥)")
        sys.exit(1)
    
    date = sys.argv[1]
    organization = sys.argv[2]
    process_type = sys.argv[3]
    
    # íƒ€ì… ê²€ì¦
    if process_type not in ['download', 'process']:
        logger.error(f"âŒ ì˜ëª»ëœ íƒ€ì…: {process_type}. 'download' ë˜ëŠ” 'process'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    try:
        datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {date}")
        sys.exit(1)
    
    logger.info(f"ğŸš€ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì¡°ì§: {organization}, íƒ€ì…: {process_type}")
    
    if process_type == 'download':
        success_count = download_all_hours(date, organization)
        logger.info(f"âœ… {date} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - {success_count}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
    elif process_type == 'process':
        logger.info(f"ğŸ”„ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì••ì¶• í•´ì œ ë° delta table ì €ì¥")
        success = process_and_save_to_delta(date, organization)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)

if __name__ == "__main__":
    main()
