import os
import subprocess
import logging
import gzip
import json
from datetime import datetime
import ast
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq
import pyarrow.fs
from deltalake import DeltaTable, write_deltalake

import pandas as pd

def _convert_all_columns_to_string(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì˜ ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    for col in df.columns:
        df[col] = df[col].astype(str)
    return df

def _filter_by_organization(df: pd.DataFrame, target_logins: list) -> pd.DataFrame:
    """ì¡°ì§ í•„í„°ë§ í•¨ìˆ˜"""
    if 'org' not in df.columns or len(df) == 0:
        return df
    
    def safe_dict_parse(x):
        if pd.isna(x) or x is None:
            return None
        if isinstance(x, str):
            try:
                return ast.literal_eval(x)
            except (ValueError, SyntaxError):
                return None
        return x
    
    df['org_parsed'] = df['org'].apply(safe_dict_parse)
    filtered_df = df[df['org_parsed'].apply(lambda x: x and isinstance(x, dict) and x.get('login') in target_logins)]
    
    if len(filtered_df) > 0:
        filtered_df = filtered_df.drop('org_parsed', axis=1)
    
    return filtered_df


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# MinIO í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
def _get_minio_client():
    """MinIO í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    try:
        from minio import Minio
        from minio.error import S3Error

        return Minio(
            os.getenv("AWS_ENDPOINT_URL", "minio:9000"),  # MinIO ì„œë²„ ì£¼ì†Œ
            access_key=os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),  # ì•¡ì„¸ìŠ¤ í‚¤
            secret_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),  # ì‹œí¬ë¦¿ í‚¤
            secure=os.getenv("MINIO_SECURE", "false").lower() == "true"  # HTTPS ì‚¬ìš© ì—¬ë¶€
        )
    except ImportError:
        logger.error("âŒ MinIO í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install minio'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        raise

def _ensure_bucket_exists(bucket_name: str):
    """ë²„í‚·ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        from minio.error import S3Error
        client = _get_minio_client()
        if not client.bucket_exists(bucket_name):
            client.make_bucket(bucket_name)
            logger.info(f"ğŸ“¦ ë²„í‚· ìƒì„± ì™„ë£Œ: {bucket_name}")
    except Exception as e:
        logger.error(f"âŒ ë²„í‚· ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def _gharchive_url_for_hour(date_str: str, hour: int) -> str:
    """
    ì£¼ì–´ì§„ ë‚ ì§œ(date_str)ì™€ ì‹œê°„(hour)ì— í•´ë‹¹í•˜ëŠ” GH Archive URLì„ ë°˜í™˜.
    ex) 2025-07-12, 0ì‹œ -> "http://data.gharchive.org/2025-07-12-0.json.gz"
    """
    return f"http://data.gharchive.org/{date_str}-{hour}.json.gz"

def _generate_urls_for_date(date_str: str) -> list[str]:
    """ë‚ ì§œ date_strì— ëŒ€í•œ 0ì‹œë¶€í„° 23ì‹œê¹Œì§€ 24ê°œì˜ URL ëª©ë¡ì„ ìƒì„±."""
    return [_gharchive_url_for_hour(date_str, h) for h in range(24)]

def _download_and_upload_to_minio(url: str, date: str, organization: str, hour: int):
    """wgetìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë°”ë¡œ MinIOì— ì—…ë¡œë“œ"""
    try:
        bucket_name = "gh-archive-raw"
        _ensure_bucket_exists(bucket_name)
        
        # tmp ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("./tmp", exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
        temp_filename = f"./tmp/{date}-{hour}.json.gz"
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        
        # wgetìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        wget_cmd = [
            "wget", 
            "-O", temp_filename,
            "--timeout=30",
            "--tries=3",
            url
        ]
        
        logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url}")
        result = subprocess.run(wget_cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"âŒ wget ì‹¤íŒ¨: {result.stderr}")
            return False
        
        # MinIOì— ì—…ë¡œë“œ
        client = _get_minio_client()
        with open(temp_filename, 'rb') as file_data:
            client.put_object(
                bucket_name,
                object_name,
                file_data,
                length=os.path.getsize(temp_filename),
                content_type="application/gzip"
            )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_filename)
        
        logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {bucket_name}/{object_name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_filename):
            os.remove(temp_filename)
        return False

def download_all_hours(date: str, organization: str):
    """í•˜ë£¨ì¹˜ ëª¨ë“  ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  MinIOì— ì—…ë¡œë“œ"""
    urls = _generate_urls_for_date(date)
    success_count = 0
    
    for hour, url in enumerate(urls):
        if _download_and_upload_to_minio(url, date, organization, hour):
            success_count += 1
    
    logger.info(f"ğŸ“Š {date} - ì´ {len(urls)}ê°œ ì¤‘ {success_count}ê°œ ì„±ê³µ")
    return success_count

def process_and_save_to_delta(date: str, organization: str):
    """MinIOì—ì„œ gz íŒŒì¼ì„ ì½ì–´ì„œ Delta Lake í…Œì´ë¸”ë¡œ ì €ì¥ (ì‹œê°„ëŒ€ë³„ ê°œë³„ ì²˜ë¦¬)"""
    try:
        import pandas as pd
        from deltalake import DeltaTable, write_deltalake
    except ImportError:
        logger.error("âŒ pandas ë˜ëŠ” deltalakeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    bucket_name = "gh-archive-raw"
    delta_bucket_name = "gh-archive-delta"
    client = _get_minio_client()
    
    # Delta Lake ë²„í‚· ìƒì„±
    _ensure_bucket_exists(delta_bucket_name)
    
    # Delta Lake ê²½ë¡œ ì„¤ì • (MinIO S3 í˜¸í™˜)
    minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
    delta_path = f"s3://{delta_bucket_name}/{organization}/{date}"
    
    # S3 í˜¸í™˜ ì„¤ì •
    storage_options = {
        "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
        "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
        "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
        "AWS_REGION": "us-east-1",  # MinIOëŠ” ê¸°ë³¸ì ìœ¼ë¡œ us-east-1 ì‚¬ìš©
        "AWS_ALLOW_HTTP": "true",
        "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
    }
    
    total_rows = 0
    success_count = 0
    
    # tmp ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./tmp", exist_ok=True)
    
    # 24ì‹œê°„ ë°ì´í„°ë¥¼ ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
    for hour in range(24):
        object_name = f"{organization}/{date}/{date}-{hour}.json.gz"
        temp_file = f"./tmp/{date}-{hour}.json.gz"
        
        try:
            # MinIOì—ì„œ gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            client.fget_object(bucket_name, object_name, temp_file)
            
            logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            
            # gz íŒŒì¼ ì••ì¶• í•´ì œ ë° JSON íŒŒì‹± (ì‹œê°„ëŒ€ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
            hour_data = []
            with gzip.open(temp_file, 'rt', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        # ê¸°ë³¸ í•„í„°ë§: orgê°€ ìˆëŠ” ë°ì´í„°ë§Œ ì²˜ë¦¬
                        if not data.get("org"):
                            continue
                        hour_data.append(data)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_file)
            
            if hour_data:
                # í˜„ì¬ ì‹œê°„ëŒ€ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_hour = pd.DataFrame(hour_data)
                
                # ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                df_hour = _convert_all_columns_to_string(df_hour)
                
                # ì²« ë²ˆì§¸ ì‹œê°„ëŒ€ëŠ” ìƒˆë¡œ ìƒì„±, ì´í›„ëŠ” append ëª¨ë“œë¡œ ì¶”ê°€
                mode = "overwrite" if hour == 0 else "append"
                print(df_hour.columns)
                # Delta Lakeì— ì €ì¥
                write_deltalake(delta_path, df_hour, mode=mode, storage_options=storage_options)
                
                # Delta Lake ì €ì¥ ì„±ê³µ í›„ MinIO raw ë²„í‚·ì—ì„œ ì›ë³¸ íŒŒì¼ ì‚­ì œ
                try:
                    client.remove_object(bucket_name, object_name)
                    logger.info(f"ğŸ—‘ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {bucket_name}/{object_name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                
                total_rows += len(df_hour)
                success_count += 1
                logger.info(f"âœ… {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ - {len(df_hour)} í–‰ ì €ì¥ë¨")
            else:
                logger.warning(f"âš ï¸ {hour}ì‹œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ {hour}ì‹œ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    if success_count == 0:
        logger.error("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logger.info(f"âœ… Delta Lake í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {delta_path} (ì´ {total_rows} í–‰, {success_count}ê°œ ì‹œê°„ëŒ€)")
    return True

def split_filtered_data_by_organization(date: str, organization: str, target_logins: list):
    """í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì¡°ì§ë³„ë¡œ ë¶„ë¦¬í•´ì„œ ì €ì¥"""
    try:
        # MinIO ì„¤ì •
        minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
        storage_options = {
            "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
            "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
            "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
            "AWS_REGION": "us-east-1",
            "AWS_ALLOW_HTTP": "true",
            "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
        }
        
        # í•„í„°ë§ëœ ë°ì´í„° ê²½ë¡œ
        source_path = f"s3://gh-archive-delta/filtered_{organization}/{date}"
        
        logger.info(f"ğŸ”„ ì¡°ì§ë³„ ë¶„ë¦¬ ì‹œì‘: {source_path}")
        
        # í•„í„°ë§ëœ ë°ì´í„° ì½ê¸°
        dt = DeltaTable(source_path, storage_options=storage_options)
        df = dt.to_pandas()
        
        logger.info(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)} í–‰")
        
        # org ì»¬ëŸ¼ íŒŒì‹±
        def safe_dict_parse(x):
            if pd.isna(x) or x is None:
                return None
            if isinstance(x, str):
                try:
                    return ast.literal_eval(x)
                except (ValueError, SyntaxError):
                    return None
            return x
        
        df['org_parsed'] = df['org'].apply(safe_dict_parse)
        
        # ê° target_loginë³„ë¡œ ë¶„ë¦¬ ì €ì¥
        success_count = 0
        for login in target_logins:
            org_df = df[df['org_parsed'].apply(lambda x: x and isinstance(x, dict) and x.get('login') == login)]
            
            if len(org_df) > 0:
                # org_parsed ì»¬ëŸ¼ ì œê±° ë° string ë³€í™˜
                org_df_clean = org_df.drop('org_parsed', axis=1)
                org_df_clean = _convert_all_columns_to_string(org_df_clean)
                
                # ì¡°ì§ë³„ ì €ì¥ ê²½ë¡œ
                output_path = f"s3://gh-archive-delta/org_{login}/{date}"
                
                # Delta Lakeì— ì €ì¥
                write_deltalake(output_path, org_df_clean, mode="overwrite", storage_options=storage_options)
                logger.info(f"âœ… {login}: {len(org_df_clean)}í–‰ ì €ì¥ ì™„ë£Œ")
                success_count += 1
            else:
                logger.warning(f"âš ï¸ {login}: ë°ì´í„° ì—†ìŒ")
        
        logger.info(f"âœ… ì¡°ì§ë³„ ë¶„ë¦¬ ì™„ë£Œ: {success_count}/{len(target_logins)}ê°œ ì¡°ì§")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ ì¡°ì§ë³„ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def filter_delta_table_by_organization(date: str, organization: str, target_logins: list):
    """Delta Lake í…Œì´ë¸”ì—ì„œ ì¡°ì§ë³„ë¡œ ë°ì´í„° í•„í„°ë§"""
    try:
        # MinIO ì„¤ì •
        minio_endpoint = os.getenv("AWS_ENDPOINT_URL", "minio:9000")
        storage_options = {
            "AWS_ENDPOINT_URL": f"http://{minio_endpoint}",
            "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
            "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin"),
            "AWS_REGION": "us-east-1",
            "AWS_ALLOW_HTTP": "true",
            "AWS_S3_ALLOW_UNSAFE_RENAME": "true"
        }
        
        # ì›ë³¸ ë° í•„í„°ë§ëœ í…Œì´ë¸” ê²½ë¡œ
        source_path = f"s3://gh-archive-delta/{organization}/{date}"
        filtered_path = f"s3://gh-archive-delta/filtered_{organization}/{date}"
        
        logger.info(f"ğŸ” í•„í„°ë§ ì‹œì‘: {source_path} -> {filtered_path}")
        
        # Delta Lake í…Œì´ë¸” ì½ê¸°
        dt = DeltaTable(source_path, storage_options=storage_options)
        df = dt.to_pandas()
        
        logger.info(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)} í–‰")
        
        # ì¡°ì§ë³„ í•„í„°ë§
        filtered_df = _filter_by_organization(df, target_logins)
        
        if len(filtered_df) > 0:
            # ëª¨ë“  ì»¬ëŸ¼ì„ string íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            filtered_df = _convert_all_columns_to_string(filtered_df)
            
            # í•„í„°ë§ëœ ë°ì´í„° ì €ì¥
            write_deltalake(filtered_path, filtered_df, mode="overwrite", storage_options=storage_options)
            logger.info(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_df)} í–‰ ì €ì¥ë¨")
            return True
        else:
            logger.warning("âš ï¸ í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        logger.error(f"âŒ í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) < 4:
        print("ì‚¬ìš©ë²•: python gh_archive_daily_collector.py <YYYY-MM-DD> <organization> <type> [target_logins]")
        print("  type: 'download', 'process', 'filter', 'split'")
        print("    - download: gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
        print("    - process: ì••ì¶• í•´ì œ í›„ delta tableì— ì €ì¥")
        print("    - filter: ì¡°ì§ë³„ í•„í„°ë§")
        print("    - split: í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì¡°ì§ë³„ë¡œ ë¶„ë¦¬")
        print("  target_logins (filter/splitìš©): ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì¡°ì§ëª… (ì˜ˆ: CausalInferenceLab,Pseudo-Lab,apache)")
        sys.exit(1)
    
    date = sys.argv[1]
    organization = sys.argv[2]
    process_type = sys.argv[3]
    
    # íƒ€ì… ê²€ì¦
    if process_type not in ['download', 'process', 'filter', 'split']:
        logger.error(f"âŒ ì˜ëª»ëœ íƒ€ì…: {process_type}. 'download', 'process', 'filter' ë˜ëŠ” 'split'ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    try:
        datetime.strptime(date, '%Y-%m-%d')
    except ValueError:
        logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {date}")
        sys.exit(1)
    
    logger.info(f"ğŸš€ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì¡°ì§: {organization}, íƒ€ì…: {process_type}")
    
    if process_type == 'download':
        success_count = download_all_hours(date, organization)
        logger.info(f"âœ… {date} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - {success_count}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
    elif process_type == 'process':
        logger.info(f"ğŸ”„ {date} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ - ì••ì¶• í•´ì œ ë° delta table ì €ì¥")
        success = process_and_save_to_delta(date, organization)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)
    elif process_type == 'filter':
        if len(sys.argv) < 5:
            logger.error("âŒ filter íƒ€ì…ì—ëŠ” target_logins íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        target_logins = sys.argv[4].split(',')
        logger.info(f"ğŸ” {date} ë°ì´í„° í•„í„°ë§ ì‹œì‘ - ëŒ€ìƒ ì¡°ì§: {target_logins}")
        success = filter_delta_table_by_organization(date, organization, target_logins)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° í•„í„°ë§ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° í•„í„°ë§ ì‹¤íŒ¨")
            sys.exit(1)
    elif process_type == 'split':
        if len(sys.argv) < 5:
            logger.error("âŒ split íƒ€ì…ì—ëŠ” target_logins íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        target_logins = sys.argv[4].split(',')
        logger.info(f"ğŸ”„ {date} ë°ì´í„° ì¡°ì§ë³„ ë¶„ë¦¬ ì‹œì‘ - ëŒ€ìƒ ì¡°ì§: {target_logins}")
        success = split_filtered_data_by_organization(date, organization, target_logins)
        if success:
            logger.info(f"âœ… {date} ë°ì´í„° ì¡°ì§ë³„ ë¶„ë¦¬ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {date} ë°ì´í„° ì¡°ì§ë³„ ë¶„ë¦¬ ì‹¤íŒ¨")
            sys.exit(1)

if __name__ == "__main__":
    main()
